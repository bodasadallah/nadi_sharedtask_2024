model_name_or_path core42/jais-13b
wandb_project nadi_sharedtask
wandb_run_name jais
use_flash_attention_2 False
checkpoint_path None
save_file None
dataset boda/nadi2024
prompt_key prompt
split train
chunk_size 100
output_dir /l/users/abdelrahman.sadallah/nadi
overwrite_output_dir False
do_train False
do_eval True
do_predict False
eval_strategy IntervalStrategy.STEPS
prediction_loss_only False
per_device_train_batch_size 4
per_device_eval_batch_size 4
per_gpu_train_batch_size None
per_gpu_eval_batch_size None
gradient_accumulation_steps 1
eval_accumulation_steps None
eval_delay 0
learning_rate 1e-06
weight_decay 0.01
adam_beta1 0.9
adam_beta2 0.999
adam_epsilon 1e-08
max_grad_norm 1.0
num_train_epochs 3.0
max_steps 100000
lr_scheduler_type SchedulerType.LINEAR
lr_scheduler_kwargs {}
warmup_ratio 0.07
warmup_steps 0
log_level passive
log_level_replica warning
log_on_each_node True
logging_dir /l/users/abdelrahman.sadallah/nadi/runs/May02_16-11-08_gpu-10
logging_strategy IntervalStrategy.STEPS
logging_first_step False
logging_steps 100
logging_nan_inf_filter True
save_strategy IntervalStrategy.STEPS
save_steps 500
save_total_limit 3
save_safetensors True
save_on_each_node False
save_only_model False
restore_callback_states_from_checkpoint False
no_cuda False
use_cpu False
use_mps_device False
seed 42
data_seed None
jit_mode_eval False
use_ipex False
bf16 False
fp16 False
fp16_opt_level O1
half_precision_backend auto
bf16_full_eval False
fp16_full_eval False
tf32 None
local_rank 0
ddp_backend None
tpu_num_cores None
tpu_metrics_debug False
debug []
dataloader_drop_last False
eval_steps 500
dataloader_num_workers 0
dataloader_prefetch_factor None
past_index -1
run_name /l/users/abdelrahman.sadallah/nadi
disable_tqdm False
remove_unused_columns True
label_names None
load_best_model_at_end True
metric_for_best_model loss
greater_is_better False
ignore_data_skip False
fsdp []
fsdp_min_num_params 0
fsdp_config {'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}
fsdp_transformer_layer_cls_to_wrap None
accelerator_config AcceleratorConfig(split_batches=False, dispatch_batches=None, even_batches=True, use_seedable_sampler=True, non_blocking=False, gradient_accumulation_kwargs=None)
deepspeed None
label_smoothing_factor 0.0
optim OptimizerNames.ADAMW_TORCH
optim_args None
adafactor False
group_by_length False
length_column_name length
report_to ['tensorboard', 'wandb']
ddp_find_unused_parameters None
ddp_bucket_cap_mb None
ddp_broadcast_buffers None
dataloader_pin_memory True
dataloader_persistent_workers False
skip_memory_metrics True
use_legacy_prediction_loop False
push_to_hub False
resume_from_checkpoint None
hub_model_id None
hub_strategy HubStrategy.EVERY_SAVE
hub_token None
hub_private_repo False
hub_always_push False
gradient_checkpointing False
gradient_checkpointing_kwargs None
include_inputs_for_metrics False
eval_do_concat_batches True
fp16_backend auto
evaluation_strategy steps
push_to_hub_model_id None
push_to_hub_organization None
push_to_hub_token None
mp_parameters 
auto_find_batch_size False
full_determinism False
torchdynamo None
ray_scope last
ddp_timeout 1800
torch_compile False
torch_compile_backend None
torch_compile_mode None
dispatch_batches None
split_batches None
include_tokens_per_second False
include_num_input_tokens_seen False
neftune_noise_alpha None
optim_target_modules None
sortish_sampler False
predict_with_generate False
generation_max_length None
generation_num_beams None
generation_config None
distributed_state Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda

_n_gpu 1
__cached__setup_devices cuda:0
deepspeed_plugin None
Loading the datasets
{'loss': 3.4259, 'grad_norm': 0.18494687974452972, 'learning_rate': 1.42836737608913e-08, 'epoch': 0.01}
{'loss': 3.427, 'grad_norm': 0.21687588095664978, 'learning_rate': 2.85673475217826e-08, 'epoch': 0.02}
{'loss': 3.4517, 'grad_norm': 0.2482605129480362, 'learning_rate': 4.28510212826739e-08, 'epoch': 0.02}
{'loss': 3.415, 'grad_norm': 0.24842044711112976, 'learning_rate': 5.71346950435652e-08, 'epoch': 0.03}
{'loss': 3.404, 'grad_norm': 0.2404046505689621, 'learning_rate': 7.14183688044565e-08, 'epoch': 0.04}
{'eval_loss': 3.9386017322540283, 'eval_runtime': 66.5498, 'eval_samples_per_second': 6.011, 'eval_steps_per_second': 1.503, 'epoch': 0.04}
{'loss': 3.4304, 'grad_norm': 0.23139320313930511, 'learning_rate': 8.57020425653478e-08, 'epoch': 0.05}
{'loss': 3.3955, 'grad_norm': 0.2558300495147705, 'learning_rate': 9.99857163262391e-08, 'epoch': 0.05}
{'loss': 3.3855, 'grad_norm': 0.2468605488538742, 'learning_rate': 1.142693900871304e-07, 'epoch': 0.06}
{'loss': 3.3674, 'grad_norm': 0.25202006101608276, 'learning_rate': 1.285530638480217e-07, 'epoch': 0.07}
{'loss': 3.3476, 'grad_norm': 0.25973019003868103, 'learning_rate': 1.42836737608913e-07, 'epoch': 0.08}
{'eval_loss': 3.8905038833618164, 'eval_runtime': 66.5475, 'eval_samples_per_second': 6.011, 'eval_steps_per_second': 1.503, 'epoch': 0.08}
{'loss': 3.3325, 'grad_norm': 0.15695048868656158, 'learning_rate': 1.5712041136980432e-07, 'epoch': 0.08}
{'loss': 3.3066, 'grad_norm': 0.25957417488098145, 'learning_rate': 1.714040851306956e-07, 'epoch': 0.09}
{'loss': 3.255, 'grad_norm': 0.2371656596660614, 'learning_rate': 1.8568775889158692e-07, 'epoch': 0.1}
{'loss': 3.2745, 'grad_norm': 0.275246798992157, 'learning_rate': 1.999714326524782e-07, 'epoch': 0.11}
{'loss': 3.2418, 'grad_norm': 0.2784741520881653, 'learning_rate': 2.142551064133695e-07, 'epoch': 0.11}
{'eval_loss': 3.787578821182251, 'eval_runtime': 66.5547, 'eval_samples_per_second': 6.01, 'eval_steps_per_second': 1.503, 'epoch': 0.11}
{'loss': 3.1715, 'grad_norm': 0.3152567148208618, 'learning_rate': 2.285387801742608e-07, 'epoch': 0.12}
{'loss': 3.1398, 'grad_norm': 0.2820327579975128, 'learning_rate': 2.428224539351521e-07, 'epoch': 0.13}
{'loss': 3.0876, 'grad_norm': 0.3624890148639679, 'learning_rate': 2.571061276960434e-07, 'epoch': 0.14}
{'loss': 3.0182, 'grad_norm': 0.3844762146472931, 'learning_rate': 2.713898014569347e-07, 'epoch': 0.14}
{'loss': 2.9473, 'grad_norm': 0.4175770580768585, 'learning_rate': 2.85673475217826e-07, 'epoch': 0.15}
{'eval_loss': 3.579380750656128, 'eval_runtime': 66.5773, 'eval_samples_per_second': 6.008, 'eval_steps_per_second': 1.502, 'epoch': 0.15}
{'loss': 2.8694, 'grad_norm': 0.35839325189590454, 'learning_rate': 2.999571489787173e-07, 'epoch': 0.16}
{'loss': 2.7852, 'grad_norm': 0.45033302903175354, 'learning_rate': 3.1424082273960863e-07, 'epoch': 0.17}
{'loss': 2.713, 'grad_norm': 0.46903324127197266, 'learning_rate': 3.285244965004999e-07, 'epoch': 0.17}
{'loss': 2.613, 'grad_norm': 0.5126137137413025, 'learning_rate': 3.428081702613912e-07, 'epoch': 0.18}
{'loss': 2.5261, 'grad_norm': 0.5114281177520752, 'learning_rate': 3.570918440222825e-07, 'epoch': 0.19}
{'eval_loss': 3.232227087020874, 'eval_runtime': 66.537, 'eval_samples_per_second': 6.012, 'eval_steps_per_second': 1.503, 'epoch': 0.19}
{'loss': 2.4132, 'grad_norm': 0.6859934329986572, 'learning_rate': 3.7137551778317383e-07, 'epoch': 0.2}
{'loss': 2.2997, 'grad_norm': 0.6491185426712036, 'learning_rate': 3.856591915440651e-07, 'epoch': 0.2}
{'loss': 2.2004, 'grad_norm': 0.6897398233413696, 'learning_rate': 3.999428653049564e-07, 'epoch': 0.21}
{'loss': 2.0769, 'grad_norm': 0.7403525114059448, 'learning_rate': 4.142265390658477e-07, 'epoch': 0.22}
{'loss': 1.9271, 'grad_norm': 0.7246798872947693, 'learning_rate': 4.28510212826739e-07, 'epoch': 0.23}
{'eval_loss': 2.7874596118927, 'eval_runtime': 66.5345, 'eval_samples_per_second': 6.012, 'eval_steps_per_second': 1.503, 'epoch': 0.23}
{'loss': 1.7333, 'grad_norm': 0.7867520451545715, 'learning_rate': 4.4279388658763035e-07, 'epoch': 0.23}
{'loss': 1.6167, 'grad_norm': 0.8062134385108948, 'learning_rate': 4.570775603485216e-07, 'epoch': 0.24}
{'loss': 1.558, 'grad_norm': 0.6432545185089111, 'learning_rate': 4.713612341094129e-07, 'epoch': 0.25}
{'loss': 1.4837, 'grad_norm': 0.7530324459075928, 'learning_rate': 4.856449078703042e-07, 'epoch': 0.26}
{'loss': 1.4594, 'grad_norm': 0.5379912257194519, 'learning_rate': 4.999285816311955e-07, 'epoch': 0.27}
{'eval_loss': 2.5915114879608154, 'eval_runtime': 66.5988, 'eval_samples_per_second': 6.006, 'eval_steps_per_second': 1.502, 'epoch': 0.27}
{'loss': 1.4011, 'grad_norm': 0.5561862587928772, 'learning_rate': 5.142122553920868e-07, 'epoch': 0.27}
{'loss': 1.4027, 'grad_norm': 0.4918193221092224, 'learning_rate': 5.284959291529781e-07, 'epoch': 0.28}
{'loss': 1.3629, 'grad_norm': 0.3725071847438812, 'learning_rate': 5.427796029138694e-07, 'epoch': 0.29}
{'loss': 1.3126, 'grad_norm': 0.3632466197013855, 'learning_rate': 5.570632766747608e-07, 'epoch': 0.3}
{'loss': 1.3233, 'grad_norm': 0.3828073740005493, 'learning_rate': 5.71346950435652e-07, 'epoch': 0.3}
{'eval_loss': 2.624791145324707, 'eval_runtime': 66.5646, 'eval_samples_per_second': 6.009, 'eval_steps_per_second': 1.502, 'epoch': 0.3}
{'loss': 1.3091, 'grad_norm': 0.3435892164707184, 'learning_rate': 5.856306241965433e-07, 'epoch': 0.31}
{'loss': 1.3011, 'grad_norm': 0.2916254699230194, 'learning_rate': 5.999142979574346e-07, 'epoch': 0.32}
{'loss': 1.259, 'grad_norm': 0.35815170407295227, 'learning_rate': 6.141979717183259e-07, 'epoch': 0.33}
{'loss': 1.2397, 'grad_norm': 0.4986349642276764, 'learning_rate': 6.284816454792173e-07, 'epoch': 0.33}
{'loss': 1.2144, 'grad_norm': 0.2076410949230194, 'learning_rate': 6.427653192401085e-07, 'epoch': 0.34}
{'eval_loss': 2.6645922660827637, 'eval_runtime': 66.5593, 'eval_samples_per_second': 6.01, 'eval_steps_per_second': 1.502, 'epoch': 0.34}
{'loss': 1.2148, 'grad_norm': 0.22079356014728546, 'learning_rate': 6.570489930009998e-07, 'epoch': 0.35}
{'loss': 1.1564, 'grad_norm': 0.3050961494445801, 'learning_rate': 6.713326667618912e-07, 'epoch': 0.36}
{'loss': 1.1844, 'grad_norm': 0.2420293539762497, 'learning_rate': 6.856163405227824e-07, 'epoch': 0.36}
{'loss': 1.1819, 'grad_norm': 0.31536194682121277, 'learning_rate': 6.999000142836738e-07, 'epoch': 0.37}
{'loss': 1.1849, 'grad_norm': 0.2529721260070801, 'learning_rate': 7.14183688044565e-07, 'epoch': 0.38}
{'eval_loss': 2.6977667808532715, 'eval_runtime': 66.5465, 'eval_samples_per_second': 6.011, 'eval_steps_per_second': 1.503, 'epoch': 0.38}
{'train_runtime': 7063.0554, 'train_samples_per_second': 56.633, 'train_steps_per_second': 14.158, 'train_loss': 2.342963360595703, 'epoch': 0.38}
Training completed. Model saved. at  /l/users/abdelrahman.sadallah/nadi/core42/jais-13b
ending 
